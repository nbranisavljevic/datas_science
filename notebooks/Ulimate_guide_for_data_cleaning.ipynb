{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a598c538-019c-4945-836e-55604ce86585",
   "metadata": {},
   "source": [
    "# Ultimate Notebook for Data Cleaning\n",
    "\n",
    "**based on** https://www.kaggle.com/discussions/getting-started/250322"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0c266-4fcb-425f-be2c-85ddcb4eb6c8",
   "metadata": {},
   "source": [
    "This notebook is developed to be a comperhensive guide for data sets cleaning and formating. \n",
    "The procedure of data cleaning is divided into two steps:\n",
    "\n",
    "**STEP 1.** Find the bad data. Bad data is divided ito several categories.\n",
    "\n",
    "- Missing data\n",
    "- Outliers\n",
    "- Contaminated data\n",
    "- Data inconsistensies\n",
    "- Invalid data\n",
    "- Data duplicates\n",
    "- Non-informative data\n",
    "- Data type issues\n",
    "\n",
    "**STEP 2** Deal with bad data. This step deals with every category mentioned above.\n",
    "\n",
    "**Note** For every step it is provided several solutions, so you need to choose the solution that is most suitabel for the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca6bd3-ec92-454f-ab48-0fde9ee8084a",
   "metadata": {},
   "source": [
    "Test data is used to ilustrate all the techniques represented in tis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770acdf-74cd-4b34-a82e-c3922417db48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "323a682e-c6df-4cac-9eb2-569b156fd136",
   "metadata": {},
   "source": [
    "**STEP 1** Find the bad data. Bad data is divided into several categories.\n",
    "\n",
    "Start data cleaning by determining what is wrong with your data: \n",
    "\n",
    "- **Missing data** Are there rows with empty values? Entire columns with no data? Which data is missing and why?\n",
    "- **Outliers** How is data distributed? Remember, visualizations are your friends. Plot outliers. Check distributions to see which groups or ranges are more heavily represented in your dataset.\n",
    "- **Contaminated data** Keep an eye out for the weird: are there impossible values? Like “date of birth: male”, “address: -1234”.\n",
    "- **Data inconsistensies** Is your data consistent? Why are the same product names written in uppercase and other times in camelCase?\n",
    "- **Data duplicates** Are there any duplicated data rows? Are there any duplicated data columns?\n",
    "- **Non-informative data** Try to find out are there columns with the same data values? \n",
    "- **Data type issues** Are there columns that should be certain data type (numeric, boolean), but is object?\n",
    "\n",
    "Wear your detective hat and jot down everything interesting, surprising or even weird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2235b7a-72d6-4c5d-8683-7e6b814fdaea",
   "metadata": {},
   "source": [
    "**STEP 2** Deal with bad data.\n",
    "\n",
    "Depending on the type of data dirt you’re facing, you’ll need different cleaning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32751d7-afd7-44cd-8f9a-250978ef10f8",
   "metadata": {},
   "source": [
    "**STEP 2.1** Missing data\n",
    "\n",
    "Sometimes you will have rows with missing values. Sometimes, almost entire columns will be empty. What to do with missing data? Some models are handlig missing data well, but some are realy bad at it. That's why you sould not ignore missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c46cec-5b99-4667-b394-351bdcaf4352",
   "metadata": {},
   "source": [
    "Start by spotting all the different disguises missing data wears. It appears in values such as *0*, *“0”*, *empty strings*, *“Not Applicable”*, *“NA”*, *“#NA”*, *None*, *NaN*, *NULL* or *Inf*. Programmers sometimes put default values instead of missing data. First check how many missing values you have in dataset, and then provide the way how you can treat them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef2e6-a477-4d99-840a-63c4498b424e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f78493a-2ab7-4b86-8db7-b14f380b1084",
   "metadata": {},
   "source": [
    "There are **3** main approaches to cleaning missing data:\n",
    "\n",
    "1. Drop rows and/or columns with missing data. If the missing data is not valuable, just drop the rows (i.e. specific customers, sensor reading, or other individual exemplars) from your analysis. If entire columns are filled with missing data, drop them as well. There is no need to analyze the column “Quantity of NewAwesomeProduct Bought” if no one has bought it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe7edb-ada8-48b2-b615-77d9015ed897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f30a151e-4f35-4d9d-86cd-69bb1417d47a",
   "metadata": {},
   "source": [
    "2. Recode missing data into a different format. Numerical computations can break down with missing data. Recoding missing values into a different column saves the day. For example, the column “payment_date” with empty rows can be recoded into a column “payed_yet” with 0 for “no” and 1 for “yes”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5ba89-222b-41a6-ba5d-3240153a49c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2516cee4-6af8-46da-a0d2-f76d4f5a309d",
   "metadata": {},
   "source": [
    "3. Fill in missing values with “best guesses.” Use moving averages and backfilling to estimate the most probable values of data at that point. This is especially crucial for time-series analyses, where missing data can distort your conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1340cf-7c81-4f14-8493-06d5fe3ffa05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3176a246-7cda-47ec-bb75-8d2a76d15e39",
   "metadata": {},
   "source": [
    "**STEP 2.2** Outliers\n",
    "\n",
    "Outliers are data points which are at an extreme. They usually have very high or very low values:\n",
    "\n",
    "An antarctic sensor reading the temperature of 100º\n",
    "A customer who buys $0.01 worth of merchandise per year\n",
    "How to interpret those?\n",
    "\n",
    "Outliers usually signify either very interesting behaviour or a broken collection process. Both are valuable information (hey, check your sensors, before checking your outliers), but proceed with cleaning only if the behaviour is actually interesting.\n",
    "\n",
    "There are three approaches to dealing with outliers:\n",
    "\n",
    "1. Remove outliers from the analysis. Having outliers can mess up your analysis by bringing the averages up or down and in general distorting your statistics. Remove them by removing the upper and lower X-percentile of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed867ed-bebc-4f6c-bee4-3262dc7377fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c40c48ed-372f-419a-b996-99f4635dcf02",
   "metadata": {},
   "source": [
    "2. Segment data so outliers are in a separate group. Put all the “normal-looking” data in one group, and outliers in another. This is especially useful for analysis of interest. You might find out that your highest paying customers, who actually buy 3 times above average, are an interesting target for marketing and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382c637-845a-4069-b0d6-c199b7a8dd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7cabdb1-dfa0-4048-9056-b91d7c17c166",
   "metadata": {},
   "source": [
    "3. Keep outliers, but use different statistical methods for analysis. Weighted means (which put more weight on the “normal” part of the distribution) and trimmed means are two common approaches of analyzing datasets with outliers, without suffering the negative consequences of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d26d95-5493-43c0-a24c-ac35588b2b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb7a9b8d-82ed-4a9f-9b6f-e560cc648e74",
   "metadata": {},
   "source": [
    "**STEP 2.3** Contaminated data\n",
    "\n",
    "Contaminated data is another red flag for your collection process.\n",
    "\n",
    "Examples of contaminated data include:\n",
    "\n",
    "Purchase information in your customer address dataset.\n",
    "Future data in your current event time-series data.\n",
    "The last one is particularly sneaky.\n",
    "\n",
    "Imagine having a row of financial trading information for each day. Columns (or features) would include the date, asset type, asking price, selling price, the difference in asking price from yesterday, the average asking price for this quarter. The average asking price for this quarter is the source of contamination. You can only compute the averages once the quarter is over, but that information would not be given to you on the trading date - thus introducing future data, which contaminates the present data.\n",
    "\n",
    "With corrupted data, there is not much you can do except for removing it. This requires a lot of domain expertise.\n",
    "\n",
    "When lacking domain knowledge, consult non-analytical members of your team. Make sure to also fix any leakages your data collection pipeline has so that the data corruption does not repeat with future data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299afef3-d4dc-4244-a101-975577fbe40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37860f53-17bf-4fc8-b366-dff0fe739099",
   "metadata": {},
   "source": [
    "**STEP 2.4** Data inconsistensies\n",
    "\n",
    "“Wait, did we sell ‘Apples’, ‘apples’, or ‘APPLES’ this month? And what is this ‘monitor stand’ for $999 under the same product ID?”\n",
    "\n",
    "You have to expect inconsistency in your data. Especially when there is a higher possibility of human error (e.g. when salespeople enter the product info on proforma invoices manually).\n",
    "\n",
    "The best way to spot inconsistent representations of the same elements in your database is to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d7f48-4d5e-4a29-8733-87cac8fc11b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9995067a-5e1c-4ed1-a426-6b15ae295a90",
   "metadata": {},
   "source": [
    "Plot bar charts per product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce61207-e632-4def-9dec-89bfbcacc4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931c3900-a868-4796-b937-a71c71f9f40e",
   "metadata": {},
   "source": [
    "Do a count of rows by category if this is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489e86f-85ef-4df6-8076-e5214a748637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfffd014-62ba-45a6-93b3-09989181ee03",
   "metadata": {},
   "source": [
    "When you spot the inconsistency, standardize all elements into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a48a6d-1890-4b8e-a585-e3e4b27c45dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f4b5c60-e126-44fb-a9d0-4947379db9cb",
   "metadata": {},
   "source": [
    "Humans might understand that ‘apples’ is the same as ‘Apples’ (capitalization) which is the same as ‘appels’ (misspelling), but computers think those three refer to three different things altogether.\n",
    "\n",
    "Lowercasing as default and correcting typos are your friends here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df602a-4165-41cf-a499-02a0c0d227a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451e0b8b-abab-4cb6-9723-1fb1ef29c1f6",
   "metadata": {},
   "source": [
    "**STEP 2.5** Invalid data\n",
    "\n",
    "Similarly, to corrupted data, invalid data is illogical.\n",
    "\n",
    "For example, users who spend -2 hours on our app, or a person whose age is 170.\n",
    "\n",
    "Unlike corrupted data, invalid data does not result from faulty collection processes, but from issues with data processing (usually during feature preparation or data cleaning).\n",
    "\n",
    "Let us walk through an example:\n",
    "\n",
    "You are preparing a report for your CEO about the average time spent in your recently launched mobile app. Everything works fine, the activities time looks great, except for a couple of rogue examples. You notice some users spent -22 hours in the app. Digging deeper, you go to the source of this anomaly. In-app time is calculated as finish_hour - start_hour. In other words, someone who started using the app at 23:00 and finished at 01:00 in the morning would have for their time_in_app -22 hours (1 - 23 = - 22).\n",
    "\n",
    "Upon realizing that, you can correct the computations to prevent such illogical data.\n",
    "\n",
    "Cleaning invalid data mostly means amending the functions and transformations which caused the data to be invalid. If this is not possible, we remove the invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c72a6-2306-48d1-a3c9-d36e11e7881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a3a8b38-f9f6-425e-8f95-fd32c2e49886",
   "metadata": {},
   "source": [
    "**STEP 2.6** Data duplicates\n",
    "\n",
    "Duplicate data means the same values repeating for an observation point.\n",
    "\n",
    "This is damaging to our analysis because it can either deflate/inflate our numbers (e.g. we count more customers than there actually are, or the average changes because some values are more often represented).\n",
    "\n",
    "There are different sources of duplicate data: Data are combined from different sources, and each source brings in the same data to our database. The user might submit information twice by clicking on the submit button. Our data collection code is off and inserts the same records multiple times.\n",
    "\n",
    "There are three ways to eliminate duplicates:\n",
    "\n",
    "1. Find the same records and delete all but one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11738613-de19-461a-910c-f2d65e8adc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cecd92-bba3-4edf-8bfc-563f1c33150d",
   "metadata": {},
   "source": [
    "2. Pairwise match records, compare them and take the most relevant one (e.g. the most recent one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88197a3f-c71e-4e28-8f7e-327924a2a82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "530071d7-995d-418f-b7e2-01e8281748c5",
   "metadata": {},
   "source": [
    "3. Combine the records into entities via clustering (e.g. the cluster of information about customer Harpreet Sahota, which has all the data associated with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1407634-e3d7-44b0-9237-fa3e26375762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b1a75b-0ea8-492d-af94-82b9c95599a8",
   "metadata": {},
   "source": [
    "**STEP 2.7** Non-informative data\n",
    "\n",
    "Sometimes the data in the column is totaly non-informative either it contains empty values or the same values for all rows. \n",
    "For example, the service is intended to be provided for the whole state, but that never happened, so the column Town is filled withe the same town name. \n",
    "\n",
    "That kind of issue is usualy treated by deleting the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e6be5-a331-40fd-9bea-6fbdd846c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b152bd95-79b9-40f0-ac29-453fb7a304ea",
   "metadata": {},
   "source": [
    "**STEP 2.8** Data type issues\n",
    "\n",
    "Depending on which data type you work with (DateTime objects, strings, integers, decimals or floats), you can encounter problems specific to data types. Always check the columns data types. It should be consistent for each column and make it consistent is part of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed338a28-0986-4856-8a7c-08f169689574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c8c7288-179b-4ba2-a98c-5a210bbc2087",
   "metadata": {},
   "source": [
    "Nevertheless, some specific dat types that should be considerd with speciall atention are:\n",
    "\n",
    "**2.7.1** Cleaning strings\n",
    "\n",
    "Strings are usually the messiest part of data cleaning because they are often human-generated and hence prone to errors. The common cleaning techniques for strings involve:\n",
    "\n",
    "1. Standardizing casing across the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78824b-d94d-4715-82a3-b0127030e908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a05179f-afb8-49bf-bf0d-80b4705636d6",
   "metadata": {},
   "source": [
    "2. Removing whitespace and newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51df48-2590-4e40-83b9-7205b3cd364c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e72103f-e264-4baa-a15f-30434374fc71",
   "metadata": {},
   "source": [
    "3. Removing stop words (for some linguistic analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e35432-7418-403d-acd2-d2d14928f6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7bd5e1-c535-4017-a57c-24da260289a5",
   "metadata": {},
   "source": [
    "4. Hot-encoding categorical variables represented as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0b46f-03a2-4e7c-a7c4-355b5302fd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49736a55-eda5-420f-8eeb-d5dc0da6279a",
   "metadata": {},
   "source": [
    "5. Correcting typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203277f4-9ee0-4988-a864-60ae9a47d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2338ce86-5b2c-48cf-b8f1-aa3cdd48af69",
   "metadata": {},
   "source": [
    "6. Standardizing encodings\n",
    "Especially the this one can cause a lot of problems. Encodings are the way of translating between the 0’s and 1’s of computers and the human- readable representation of text. And as there are different languages, there are different encodings. Everyone has seen strings of the type that our browser or computer could not decode the string. It is the same as trying to play a cassette on your gramophone. Both are made for music, but they represent it in different ways. When in doubt, go for UTF-8 as your encoding standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3745124-de0b-4e22-aae3-26e4c5132b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7224a116-bd93-4911-81c1-4db0a70ce5f4",
   "metadata": {},
   "source": [
    "**2.7.2** Cleaning date and time\n",
    "\n",
    "Dates and time can be tricky. Sometimes the error is not apparent until doing computations (like the activity duration example above) on date and times.\n",
    "\n",
    "The cleaning process involves:\n",
    "\n",
    "1. Making sure that all your dates and times are either a DateTime object or a Unix timestamp (via type coercion). Do not be tricked by strings pretending to be a DateTime object, like “24 Oct 2019”. Check for data type and coerce where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503e1e7-f0ff-4690-b629-a62386d5cb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fcd98c6-868c-4f7d-8083-71210eb6e8ae",
   "metadata": {},
   "source": [
    "2. Internationalization and time zones. DateTime objects are often recorded with the time zone or without one. Either of those can cause problems. If you are doing region-specific analysis, make sure to have DateTime in the correct timezone. If you do not care about internationalization, convert all DateTime objects to your timezone.                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf79a90-2d15-4bfd-86f6-a933d68770fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3054eae0-49c6-45d3-b840-d78200d501ec",
   "metadata": {},
   "source": [
    "This was all about the data cleaning for now. This is a project that is going to be developed further, so feel free to contribute:) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9329053-d3de-46b6-8bd9-dab63e084eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
